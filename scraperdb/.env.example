# TenderVendor Pipeline Configuration
# Copy this file to .env and update the values

# =============================================================================
# DATABASE CONFIGURATION
# =============================================================================

# MongoDB connection
MONGO_URI=mongodb://localhost:27017
DB_NAME=tender_db

# ChromaDB persistent storage path
CHROMA_PATH=./data/chroma

# =============================================================================
# EMBEDDING MODEL CONFIGURATION
# =============================================================================

# Embedding model (options: BAAI/bge-m3, BAAI/bge-base-en-v1.5, sentence-transformers/all-MiniLM-L6-v2)
# BAAI/bge-m3: Best quality, slower, multilingual support
# BAAI/bge-base-en-v1.5: Good balance of speed and quality
# all-MiniLM-L6-v2: Fastest, good for testing
EMBEDDING_MODEL_NAME=BAAI/bge-m3

# Embedding provider (auto, flagembedding, sentence-transformers)
EMBEDDING_PROVIDER=auto

# Use FP16 for faster inference on CUDA GPUs
EMBEDDING_USE_FP16=true

# =============================================================================
# TENDER PROCESSING CONFIGURATION
# =============================================================================

# Tender chunk settings
CHUNK_SIZE=500
CHUNK_OVERLAP=80
BATCH_SIZE=64

# Tender ChromaDB collection name
TENDER_CHROMA_COLLECTION=tender_embeddings

# =============================================================================
# PROFILE PROCESSING CONFIGURATION (OPTIMIZED)
# =============================================================================

# Profile chunk settings (larger chunks for profiles)
PROFILE_CHUNK_SIZE=800
PROFILE_CHUNK_OVERLAP=100
PROFILE_BATCH_SIZE=64

# Number of representative embeddings for multi-vector search
# Higher = better coverage but more storage/computation
PROFILE_N_REPRESENTATIVES=5

# Profile ChromaDB collection name
PROFILE_CHROMA_COLLECTION=company_profiles

# Maximum file size (MB) for synchronous processing
# Files larger than this are processed asynchronously
PROFILE_SYNC_MAX_MB=5.0

# =============================================================================
# SMART EXTRACTION CONFIGURATION
# =============================================================================

# Enable smart section extraction (reduces 200 pages -> 20-30 pages)
USE_SMART_EXTRACTION=true

# Maximum characters to extract from profiles
SMART_EXTRACTION_MAX_CHARS=100000

# =============================================================================
# PARALLEL PROCESSING CONFIGURATION
# =============================================================================

# Maximum parallel PDF processing workers
MAX_PARALLEL_PDFS=4

# =============================================================================
# CACHING CONFIGURATION
# =============================================================================

# Enable file-based Docling output cache
ENABLE_DOCLING_CACHE=true

# Directory for Docling cache files (leave empty to disable file cache)
DOCLING_CACHE_DIR=./data/cache/docling

# Cache TTL in hours (168 = 1 week)
CACHE_TTL_HOURS=168

# =============================================================================
# LLM EXTRACTION CONFIGURATION (OPTIONAL)
# =============================================================================

# Enable LLM-powered profile extraction for structured data
USE_LLM_EXTRACTION=true

# Gemini API key
GEMINI_API_KEY=AIzaSyBTn3g2sZdQQVWIJE7K6jWIFxXF2l3cSKM


# Gemini model (use gemini-1.5-flash or gemini-2.0-flash-exp)
GEMINI_MODEL=gemini-2.5-flash

# Maximum input characters for LLM extraction
LLM_MAX_INPUT_CHARS=80000

# =============================================================================
# SEARCH CONFIGURATION
# =============================================================================

# Enable hybrid multi-vector search (recommended)
USE_HYBRID_SEARCH=true

# =============================================================================
# PIPELINE CONFIGURATION
# =============================================================================

# Lock file to prevent concurrent pipeline runs
PIPELINE_LOCK_FILE=./data/pipeline.lock

# Default document collection for Docling processing
DOCS_COLLECTION=tender_documents

# Maximum documents to process per pipeline run
DOCLING_LIMIT=50
